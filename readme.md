# Deep Learning

**Deep Learning (DL)** is a branch of machine learning that focuses on using multi-layered artificial neural networks to model and learn complex relationships within data. It automatically extracts and represents high-level features from raw inputs, such as images, text, or sound, without requiring manual feature engineering. Deep learning systems consist of multiple processing layers (or “hidden layers”) that progressively transform the input data into more abstract and meaningful representations. These models are capable of handling large datasets and are particularly effective in tasks like image recognition, speech processing, and natural language understanding.

# Types of Deep Learning


The main **types of Deep Learning (DL)** models are based on the structure and purpose of their neural networks. The most common types are:

---

### **1. Feedforward Neural Network (FNN)**

* The simplest type of neural network.
* Information moves only in one direction — from input to output.
* Used for basic prediction and classification tasks.

**Example:** Handwritten digit recognition (like MNIST dataset).

---

### **2. Convolutional Neural Network (CNN)**

* Designed to process **image and visual data**.
* Uses **convolutional layers** to automatically detect spatial features like edges, textures, and shapes.
* Highly effective in computer vision tasks.

**Applications:** Image classification, object detection, facial recognition.

---

### **3. Recurrent Neural Network (RNN)**

* Designed to handle **sequential or time-series data**.
* Has connections that form cycles, allowing memory of previous inputs.
* Useful for tasks where context or order matters.

**Applications:** Speech recognition, text generation, stock prediction.

---

### **4. Long Short-Term Memory (LSTM)**

* A special type of RNN that can **remember information for long periods**.
* Solves the vanishing gradient problem in regular RNNs.

**Applications:** Language modeling, translation, and sequence prediction.

---

### **5. Gated Recurrent Unit (GRU)**

* A simplified version of LSTM with fewer parameters but similar performance.
* Faster and easier to train than LSTMs.

**Applications:** Chatbots, time-series forecasting.

---

### **6. Autoencoder**

* Used for **unsupervised learning**.
* Learns to compress data (encoding) and then reconstruct it (decoding).
* Helpful in dimensionality reduction and noise removal.

**Applications:** Image compression, anomaly detection.

---

### **7. Generative Adversarial Network (GAN)**

* Consists of two networks — **Generator** and **Discriminator** — that compete with each other.
* Used to generate new, realistic data similar to the training data.

**Applications:** Image generation, deepfake creation, data augmentation.

---

### **8. Transformer**

* Works on the principle of **attention mechanism**, allowing the model to focus on important parts of input sequences.
* Outperforms RNNs and LSTMs for text and sequential data.

**Applications:** Language translation, chatbots, large language models (like GPT).



